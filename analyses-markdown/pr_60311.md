# PR #60311: threads: Implement asymmetric atomic fences

## Metadata

- **Author**: Keno
- **URL**: https://github.com/JuliaLang/julia/pull/60311
- **Merged**: 2025-12-15 12:11 UTC
- **Diff**: https://github.com/JuliaLang/julia/pull/60311.diff

## Scope

### Files Touched
- `Compiler/src/tfuncs.jl`
- `NEWS.md`
- `base/asyncevent.jl`
- `base/atomics.jl`
- `doc/src/base/multi-threading.md`
- `src/ast.c`
- `src/intrinsics.cpp`
- `src/intrinsics.h`
- `src/jl_exported_funcs.inc`
- `src/julia_internal.h`
- `src/runtime_intrinsics.c`
- `src/signals-mach.c`
- `src/signals-unix.c`
- `src/signals-win.c`
- `test/intrinsics.jl`
- `test/threads_exec.jl`

### Components
- Compiler.tfuncs
- Base.Threads
- Codegen.Intrinsics
- Runtime.Intrinsics
- Signals

### Pipeline Stages
- TypeInference
- Codegen
- Runtime
- Threading

## Analysis

### Intent

Add asymmetric atomic fences with light/heavy variants, extend Core.Intrinsics.atomic_fence to accept a syncscope, and provide OS-specific heavy-fence implementations.

**Related Issues:**
- https://github.com/JuliaLang/julia/pull/60311

### Direct Changes

#### 1. Expose new public threading APIs for asymmetric fences and route the existing fence through a system syncscope.

**Component**: Base.Threads

<details>
<summary>Evidence</summary>

**base/atomics.jl:317-356**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/base/atomics.jl#L317-L356)
```julia
"""
    Threads.atomic_fence()

Insert a sequential-consistency memory fence

Inserts a memory fence with sequentially-consistent ordering
semantics. There are algorithms where this is needed, i.e. where an
acquire/release ordering is insufficient.

This is likely a very expensive operation. Given that all other atomic
operations in Julia already have acquire/release semantics, explicit
fences should not be necessary in most cases.

For further details, see LLVM's `fence` instruction.
"""
atomic_fence() = Core.Intrinsics.atomic_fence(:sequentially_consistent, :system)

"""
    Threads.atomic_fence_light()

Insert the light side of an asymmetric sequential-consistency memory fence.
Asymmetric memory fences are useful in scenarios where one side of the
synchronization runs significantly less often than the other side. Use this
function on the side that runs often and [`atomic_fence_heavy`](@ref) on the
side that runs rarely.

On supported operating systems and architectures this fence is cheaper than
`Threads.atomic_fence()`, but synchronizes only with [`atomic_fence_heavy`](@ref)
calls from other threads.
"""
atomic_fence_light() = Core.Intrinsics.atomic_fence(:sequentially_consistent, :singlethread)

"""
    Threads.atomic_fence_heavy()

Insert the heavy side of an asymmetric sequential-consistency memory fence.
Use this function on the side that runs rarely.
See [`atomic_fence_light`](@ref) for more details.
"""
atomic_fence_heavy() = ccall(:jl_membarrier, Cvoid, ())
```

</details>

#### 2. Threading code that relied on Core.Intrinsics.atomic_fence now passes an explicit syncscope symbol.

**Component**: Base.Threading

<details>
<summary>Evidence</summary>

**base/asyncevent.jl:164-169**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/base/asyncevent.jl#L164-L169)
```julia
function _trywait(t::Union{Timer, AsyncCondition})
    set = t.set
    if set
        # full barrier now for AsyncCondition
        t isa Timer || Core.Intrinsics.atomic_fence(:acquire_release, :system)
    else
```

</details>

#### 3. Extend the atomic_fence intrinsic to accept a syncscope and select LLVM syncscope at compile time when possible.

**Component**: Codegen.Intrinsics

<details>
<summary>Evidence</summary>

**src/intrinsics.cpp:915-938**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/intrinsics.cpp#L915-L938)
```julia
static jl_cgval_t emit_atomicfence(jl_codectx_t &ctx, ArrayRef<jl_cgval_t> argv)
{
    const jl_cgval_t &ord = argv[0];
    const jl_cgval_t &ssid_arg = argv[1];
    llvm::SyncScope::ID ssid = llvm::SyncScope::System;
    if (!ssid_arg.constant || !jl_is_symbol(ssid_arg.constant) ||
        ((jl_sym_t*)ssid_arg.constant != jl_singlethread_sym &&
            (jl_sym_t*)ssid_arg.constant != jl_system_sym)) {
        return emit_runtime_call(ctx, atomic_fence, argv, 2);
    }
    if ((jl_sym_t*)ssid_arg.constant == jl_singlethread_sym)
        ssid = llvm::SyncScope::SingleThread;
    if (ord.constant && jl_is_symbol(ord.constant)) {
        enum jl_memory_order order = jl_get_atomic_order((jl_sym_t*)ord.constant, true, true);
        if (order == jl_memory_order_invalid) {
            emit_atomic_error(ctx, "invalid atomic ordering");
            return jl_cgval_t(); // unreachable
        }
        if (order > jl_memory_order_monotonic)
            ctx.builder.CreateFence(get_llvm_atomic_order(order), ssid);
        return ghostValue(ctx, jl_nothing_type);
    }
    return emit_runtime_call(ctx, atomic_fence, argv, 2);
}
```

</details>

#### 4. Runtime intrinsic validation adds syncscope checking, a single-thread compiler barrier, and error reporting for invalid syncscope symbols.

**Component**: Runtime.Intrinsics

<details>
<summary>Evidence</summary>

**src/runtime_intrinsics.c:625-638**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/runtime_intrinsics.c#L625-L638)
```julia
JL_DLLEXPORT jl_value_t *jl_atomic_fence(jl_value_t *order_sym, jl_value_t *syncscope_sym)
{
    JL_TYPECHK(fence, symbol, order_sym);
    JL_TYPECHK(fence, symbol, syncscope_sym);
    enum jl_memory_order order = jl_get_atomic_order_checked((jl_sym_t*)order_sym, 1, 1);
    if ((jl_sym_t*)syncscope_sym == jl_singlethread_sym) {
        asm volatile ("" : : : "memory");
        return jl_nothing;
    } else if ((jl_sym_t*)syncscope_sym != jl_system_sym) {
        jl_error("atomic_fence: invalid syncscope");
    }
    if (order > jl_memory_order_monotonic)
        jl_fence();
    return jl_nothing;
}
```

</details>

#### 5. Introduce jl_membarrier with OS-specific heavy-fence implementations (Linux/FreeBSD, macOS, Windows).

**Component**: Runtime.Signals

<details>
<summary>Evidence</summary>

**src/signals-unix.c:1367-1445**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/signals-unix.c#L1367-L1445)
```julia
enum membarrier_implementation {
    MEMBARRIER_IMPLEMENTATION_UNKNOWN        = 0,
    MEMBARRIER_IMPLEMENTATION_SYS_MEMBARRIER = 1,
    MEMBARRIER_IMPLEMENTATION_MPROTECT       = 2,
    MEMBARRIER_IMPLEMENTATION_THREAD_SUSPEND = 3
};

static _Atomic(enum membarrier_implementation) membarrier_impl = MEMBARRIER_IMPLEMENTATION_UNKNOWN;

static enum membarrier_implementation jl_init_membarrier(void) {
#ifdef HAVE_MEMBARRIER_SYSCALL
    int ret = membarrier(MEMBARRIER_CMD_QUERY, 0);
    int needed = MEMBARRIER_CMD_PRIVATE_EXPEDITED | MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED;
    if (ret > 0 && ((ret & needed) == needed)) {
        // supported
        if (membarrier(MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED, 0) == 0) {
            // working
            jl_atomic_store_relaxed(&membarrier_impl, MEMBARRIER_IMPLEMENTATION_SYS_MEMBARRIER);
            return MEMBARRIER_IMPLEMENTATION_SYS_MEMBARRIER;
        }
    }
#endif
    // The mprotect fallback is known not to work on AArch64, so skip it there
#if !defined(_CPU_AARCH64_) && !defined(_CPU_ARM_)
    if (jl_init_mprotect_membarrier()) {
        jl_atomic_store_relaxed(&membarrier_impl, MEMBARRIER_IMPLEMENTATION_MPROTECT);
        return MEMBARRIER_IMPLEMENTATION_MPROTECT;
    }
#endif
    // Fall back to thread suspension (sound but slow)
    jl_atomic_store_relaxed(&membarrier_impl, MEMBARRIER_IMPLEMENTATION_THREAD_SUSPEND);
    return MEMBARRIER_IMPLEMENTATION_THREAD_SUSPEND;
}

JL_DLLEXPORT void jl_membarrier(void) {
    enum membarrier_implementation impl = jl_atomic_load_relaxed(&membarrier_impl);
    if (impl == MEMBARRIER_IMPLEMENTATION_UNKNOWN) {
        impl = jl_init_membarrier();
    }
    switch (impl) {
#ifdef HAVE_MEMBARRIER_SYSCALL
    case MEMBARRIER_IMPLEMENTATION_SYS_MEMBARRIER: {
        int ret = membarrier(MEMBARRIER_CMD_PRIVATE_EXPEDITED, 0);
        assert(ret == 0);
        (void)ret;
        break;
    }
#endif
#if !defined(_CPU_AARCH64_) && !defined(_CPU_ARM_)
    case MEMBARRIER_IMPLEMENTATION_MPROTECT:
        jl_mprotect_membarrier();
        break;
#endif
    case MEMBARRIER_IMPLEMENTATION_THREAD_SUSPEND:
        jl_thread_suspend_membarrier();
        break;
    default:
        abort();
    }
}
```

**src/signals-mach.c:896-937**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/signals-mach.c#L896-L937)
```julia
// The mprotect implementation in signals-unix.c does not work on macOS/aarch64, as mentioned.
// This implementation comes from dotnet, but is similarly dependent on undocumented behavior of the OS.
// Copyright (c) .NET Foundation and Contributors
// MIT LICENSE
JL_DLLEXPORT void jl_membarrier(void) {
    uintptr_t sp;
    uintptr_t registerValues[128];
    kern_return_t machret;

    // Iterate through each of the threads in the list.
    int nthreads = jl_atomic_load_acquire(&jl_n_threads);
    for (int tid = 0; tid < nthreads; tid++) {
        jl_ptls_t ptls2 = jl_atomic_load_relaxed(&jl_all_tls_states)[tid];
        thread_act_t thread = pthread_mach_thread_np(ptls2->system_id);
        if (__builtin_available (macOS 10.14, iOS 12, tvOS 9, *))
        {
            // Request the threads pointer values to force the thread to emit a memory barrier
            size_t registers = 128;
            machret = thread_get_register_pointer_values(thread, &sp, &registers, registerValues);
        }
        else
        {
            // fallback implementation for older OS versions
#if defined(_CPU_X86_64_)
            x86_thread_state64_t threadState;
            mach_msg_type_number_t count = x86_THREAD_STATE64_COUNT;
            machret = thread_get_state(thread, x86_THREAD_STATE64, (thread_state_t)&threadState, &count);
#elif defined(_CPU_AARCH64_)
            arm_thread_state64_t threadState;
            mach_msg_type_number_t count = ARM_THREAD_STATE64_COUNT;
            machret = thread_get_state(thread, ARM_THREAD_STATE64, (thread_state_t)&threadState, &count);
#else
            #error Unexpected architecture
#endif
        }

        if (machret == KERN_INSUFFICIENT_BUFFER_SIZE)
        {
            HANDLE_MACH_ERROR("thread_get_register_pointer_values()", machret);
        }
    }
}
```

**src/signals-win.c:668-670**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/signals-win.c#L668-L670)
```julia
JL_DLLEXPORT void jl_membarrier(void) {
    FlushProcessWriteBuffers();
}
```

</details>

#### 6. Compiler tfuncs and intrinsic metadata updated for the new atomic_fence arity and syncscope argument.

**Component**: Compiler.tfuncs

<details>
<summary>Evidence</summary>

**Compiler/src/tfuncs.jl:719-761**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/Compiler/src/tfuncs.jl#L719-L761)
```julia
@nospecs function atomic_fence_tfunc(ùïÉ::AbstractLattice, order, syncscope)
    return Nothing
end
@nospecs function atomic_pointerref_tfunc(ùïÉ::AbstractLattice, a, order)
    return pointer_eltype(a)
end
@nospecs function atomic_pointerset_tfunc(ùïÉ::AbstractLattice, a, v, order)
    return a
end
@nospecs function atomic_pointerswap_tfunc(ùïÉ::AbstractLattice, a, v, order)
    return pointer_eltype(a)
end
@nospecs function atomic_pointermodify_tfunc(ùïÉ::AbstractLattice, ptr, op, v, order)
    a = widenconst(ptr)
    if !has_free_typevars(a)
        unw = unwrap_unionall(a)
        if isa(unw, DataType) && unw.name === Ptr.body.name
            T = unw.parameters[1]
            # note: we could sometimes refine this to a PartialStruct if we analyzed `op(T, T)::T`
            valid_as_lattice(T, true) || return Bottom
            return rewrap_unionall(Pair{T, T}, a)
        end
    end
    return Pair
end
@nospecs function atomic_pointerreplace_tfunc(ùïÉ::AbstractLattice, ptr, x, v, success_order, failure_order)
    a = widenconst(ptr)
    if !has_free_typevars(a)
        unw = unwrap_unionall(a)
        if isa(unw, DataType) && unw.name === Ptr.body.name
            T = unw.parameters[1]
            valid_as_lattice(T) || return Bottom
            return rewrap_unionall(ccall(:jl_apply_cmpswap_type, Any, (Any,), T), a)
        end
    end
    return ccall(:jl_apply_cmpswap_type, Any, (Any,), T) where T
end
add_tfunc(add_ptr, 2, 2, pointerarith_tfunc, 1)
add_tfunc(sub_ptr, 2, 2, pointerarith_tfunc, 1)
add_tfunc(pointerref, 3, 3, pointerref_tfunc, 4)
add_tfunc(pointerset, 4, 4, pointerset_tfunc, 5)
add_tfunc(atomic_fence, 2, 2, atomic_fence_tfunc, 4)
add_tfunc(atomic_pointerref, 2, 2, atomic_pointerref_tfunc, 4)
add_tfunc(atomic_pointerset, 3, 3, atomic_pointerset_tfunc, 5)
add_tfunc(atomic_pointerswap, 3, 3, atomic_pointerswap_tfunc, 5)
```

</details>

#### 7. Tests updated to exercise new syncscope signature and asymmetric fence behavior.

**Component**: Tests.Threading

<details>
<summary>Evidence</summary>

**test/intrinsics.jl:397-405**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/test/intrinsics.jl#L397-L405)
```julia
using Base.Experimental: @force_compile
@test_throws ConcurrencyViolationError("invalid atomic ordering") (@force_compile; Core.Intrinsics.atomic_fence(:u, :system)) === nothing
@test_throws ConcurrencyViolationError("invalid atomic ordering") (@force_compile; Core.Intrinsics.atomic_fence(Symbol("u", "x"), :system)) === nothing
@test_throws ConcurrencyViolationError("invalid atomic ordering") Core.Intrinsics.atomic_fence(Symbol("u", "x"), :system) === nothing
for order in (:not_atomic, :monotonic, :acquire, :release, :acquire_release, :sequentially_consistent)
    @test Core.Intrinsics.atomic_fence(order, :system) === nothing
    @test (order -> Core.Intrinsics.atomic_fence(order, :system))(order) === nothing
    @test Base.invokelatest(@eval () -> Core.Intrinsics.atomic_fence($(QuoteNode(order)), :system)) === nothing
end
```

**test/threads_exec.jl:467-515**
[View on GitHub](https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/test/threads_exec.jl#L467-L515)
```julia
# Test asymmetric thread fences
struct AsymmetricFenceTestData
    n::Int
    x::AtomicMemory{Int}
    y::AtomicMemory{Int}
    read_x::AtomicMemory{Int}
    read_y::AtomicMemory{Int}
end
function test_asymmetric_fence(data::AsymmetricFenceTestData, cond1, cond2, threadid, it)
    if (threadid % 2) == 0
        @atomic :monotonic data.x[it] = 1
        Threads.atomic_fence_heavy()
        @atomic :monotonic data.read_y[it] = @atomic :monotonic data.y[it]
        wait(cond1)
        notify(cond2)
    else
        @atomic :monotonic data.y[it] = 1
        Threads.atomic_fence_light()
        @atomic :monotonic data.read_x[it] = @atomic :monotonic data.x[it]
        notify(cond1)
        wait(cond2)
    end
end
function test_asymmetric_fence(data::AsymmetricFenceTestData, cond1, cond2, threadid)
    for i = 1:data.n
        test_asymmetric_fence(data, cond1, cond2, threadid, i)
    end
end
function test_asymmetric_fence()
    asymmetric_test_count = 200_000
    cond1 = Threads.Event(true)
    cond2 = Threads.Event(true)
    data = AsymmetricFenceTestData(asymmetric_test_count,
                                   AtomicMemory{Int}(undef, asymmetric_test_count),
                                   AtomicMemory{Int}(undef, asymmetric_test_count),
                                   AtomicMemory{Int}(undef, asymmetric_test_count),
                                   AtomicMemory{Int}(undef, asymmetric_test_count))
    for i = 1:asymmetric_test_count
        @atomic :monotonic data.x[i] = 0
        @atomic :monotonic data.y[i] = 0
        @atomic :monotonic data.read_x[i] = typemax(Int)
        @atomic :monotonic data.read_y[i] = typemax(Int)
    end
    t1 = @Threads.spawn test_asymmetric_fence(data, cond1, cond2, 1)
    t2 = @Threads.spawn test_asymmetric_fence(data, cond1, cond2, 2)
    wait(t1); wait(t2)
    @test !any((data.read_x .== 0) .& (data.read_y .== 0))
end
test_asymmetric_fence()
```

</details>

### Secondary Effects

#### Light fences only emit a compiler barrier (single-thread syncscope) and therefore synchronize only with heavy fences, not with ordinary fences.

**Likelihood**: high | **Impact**: medium

<details>
<summary>Mechanism</summary>

```
Threads.atomic_fence_light()  [base/atomics.jl:334-347]
  -> Core.Intrinsics.atomic_fence(:sequentially_consistent, :singlethread)  [base/atomics.jl:347]
  -> emit_atomicfence() chooses llvm::SyncScope::SingleThread and uses CreateFence(order, ssid)  [src/intrinsics.cpp:915-935]
  -> runtime fallback treats :singlethread as `asm volatile ("" : : : "memory")`  [src/runtime_intrinsics.c:625-633]
```
</details>

**Downstream Surfaces:**
- Base.Threads.atomic_fence_light
- Core.Intrinsics.atomic_fence(syncscope=:singlethread)
- LLVM IR fence syncscope=singlethread

#### Heavy fence path may trigger system calls or thread suspension, causing cross-thread latency spikes on some OS/arch combinations.

**Likelihood**: medium | **Impact**: medium

<details>
<summary>Mechanism</summary>

```
Threads.atomic_fence_heavy()  [base/atomics.jl:349-356]
  -> ccall(:jl_membarrier, Cvoid, ())  [base/atomics.jl:356]
  -> jl_membarrier() selects implementation (membarrier syscall, mprotect, or thread suspend)  [src/signals-unix.c:1367-1445]
  -> macOS uses Mach thread register queries to induce barriers  [src/signals-mach.c:896-936]
  -> Windows uses FlushProcessWriteBuffers  [src/signals-win.c:668-670]
```
</details>

**Downstream Surfaces:**
- Threads.atomic_fence_heavy
- Runtime jl_membarrier
- Schedulers sensitive to thread suspension

#### Invalid syncscope symbols now raise errors in runtime intrinsics, which may surface in tooling that calls Core.Intrinsics.atomic_fence directly.

**Likelihood**: low | **Impact**: low

<details>
<summary>Mechanism</summary>

```
Core.Intrinsics.atomic_fence(order, syncscope)
  -> jl_atomic_fence(order_sym, syncscope_sym)  [src/runtime_intrinsics.c:625-638]
  -> jl_error("atomic_fence: invalid syncscope") for non-:system/:singlethread symbols  [src/runtime_intrinsics.c:633-634]
```
</details>

**Downstream Surfaces:**
- Core.Intrinsics.atomic_fence
- Compiler plugins emitting intrinsics

### Compatibility

#### Internal API Changes
- **Core.Intrinsics.atomic_fence arity**: Intrinsic now takes (order, syncscope) instead of a single order argument; metadata updated in intrinsics.h and codegen assert expects nargs==2.
- **jl_atomic_fence signature**: Runtime API now expects (order, syncscope) and validates syncscope symbol before issuing a fence.
- **jl_membarrier export**: New runtime export available for heavy fence path.

#### Behavioral Changes
- Base.Threads.atomic_fence now uses syncscope :system explicitly, and a new :singlethread syncscope is introduced for light fences. *(Impact: Direct calls to Core.Intrinsics.atomic_fence must include a valid syncscope or they will error.)*
- Asymmetric fence pair guarantees are now tested via a two-thread atomic memory litmus. *(Impact: Code that expects symmetric fence behavior should prefer Threads.atomic_fence() instead of light/heavy pair.)*

### Performance

**Compile Time:**
- {'item': 'ESTIMATED: negligible compile-time impact; extra intrinsic argument and syncscope validation are constant-time in codegen.'}

**Runtime:**
- {'item': 'ESTIMATED: atomic_fence_light emits only a compiler barrier (singlethread syncscope) and should be much cheaper than a full system fence.'}
- {'item': 'ESTIMATED: atomic_fence_heavy may perform system calls or thread suspension, so per-call latency can be high; use on cold paths.'}

### Risk Assessment

**Level**: medium

**Rationale:**
- New syncscope argument touches codegen, runtime intrinsics, and exported C API; mistakes could surface as invalid call errors or wrong fence strength.
- Heavy fence implementations depend on OS-specific behavior (membarrier/mprotect/thread suspension) with potential latency and portability implications.

### Recommendations

- Audit downstream packages that call Core.Intrinsics.atomic_fence directly to pass an explicit syncscope (:system for ordinary fences).
- Prefer Threads.atomic_fence_light/heavy only when the synchronization pattern is asymmetric; otherwise keep using Threads.atomic_fence().
- Consider adding microbenchmarks for jl_membarrier implementations to detect regressions in latency-sensitive workloads.
