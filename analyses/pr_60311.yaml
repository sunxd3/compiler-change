schema_version: "1.0"
pr:
  number: 60311
  title: "threads: Implement asymmetric atomic fences"
  url: "https://github.com/JuliaLang/julia/pull/60311"
  author: "Keno"
  labels: []
  merged_at: "2025-12-15T12:11:49Z"
  merge_commit_sha: "b399d93b9eaf5bd1f223fefbb667e111a824c151"
  diff_url: "https://github.com/JuliaLang/julia/pull/60311.diff"
scope:
  files_touched:
    - "Compiler/src/tfuncs.jl"
    - "NEWS.md"
    - "base/asyncevent.jl"
    - "base/atomics.jl"
    - "doc/src/base/multi-threading.md"
    - "src/ast.c"
    - "src/intrinsics.cpp"
    - "src/intrinsics.h"
    - "src/jl_exported_funcs.inc"
    - "src/julia_internal.h"
    - "src/runtime_intrinsics.c"
    - "src/signals-mach.c"
    - "src/signals-unix.c"
    - "src/signals-win.c"
    - "test/intrinsics.jl"
    - "test/threads_exec.jl"
  components:
    - "Compiler.tfuncs"
    - "Base.Threads"
    - "Codegen.Intrinsics"
    - "Runtime.Intrinsics"
    - "Signals"
  pipeline_stages:
    - "TypeInference"
    - "Codegen"
    - "Runtime"
    - "Threading"
analysis:
  intent:
    summary: "Add asymmetric atomic fences with light/heavy variants, extend Core.Intrinsics.atomic_fence to accept a syncscope, and provide OS-specific heavy-fence implementations."
    issue_links:
      - "https://github.com/JuliaLang/julia/pull/60311"
  direct_changes:
    - summary: "Expose new public threading APIs for asymmetric fences and route the existing fence through a system syncscope."
      component: "Base.Threads"
      evidence:
        - source: "code"
          path: "base/atomics.jl"
          loc: "317-356"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/base/atomics.jl#L317-L356"
          snippet: |
            """
                Threads.atomic_fence()

            Insert a sequential-consistency memory fence

            Inserts a memory fence with sequentially-consistent ordering
            semantics. There are algorithms where this is needed, i.e. where an
            acquire/release ordering is insufficient.

            This is likely a very expensive operation. Given that all other atomic
            operations in Julia already have acquire/release semantics, explicit
            fences should not be necessary in most cases.

            For further details, see LLVM's `fence` instruction.
            """
            atomic_fence() = Core.Intrinsics.atomic_fence(:sequentially_consistent, :system)

            """
                Threads.atomic_fence_light()

            Insert the light side of an asymmetric sequential-consistency memory fence.
            Asymmetric memory fences are useful in scenarios where one side of the
            synchronization runs significantly less often than the other side. Use this
            function on the side that runs often and [`atomic_fence_heavy`](@ref) on the
            side that runs rarely.

            On supported operating systems and architectures this fence is cheaper than
            `Threads.atomic_fence()`, but synchronizes only with [`atomic_fence_heavy`](@ref)
            calls from other threads.
            """
            atomic_fence_light() = Core.Intrinsics.atomic_fence(:sequentially_consistent, :singlethread)

            """
                Threads.atomic_fence_heavy()

            Insert the heavy side of an asymmetric sequential-consistency memory fence.
            Use this function on the side that runs rarely.
            See [`atomic_fence_light`](@ref) for more details.
            """
            atomic_fence_heavy() = ccall(:jl_membarrier, Cvoid, ())
    - summary: "Threading code that relied on Core.Intrinsics.atomic_fence now passes an explicit syncscope symbol."
      component: "Base.Threading"
      evidence:
        - source: "code"
          path: "base/asyncevent.jl"
          loc: "164-169"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/base/asyncevent.jl#L164-L169"
          snippet: |
            function _trywait(t::Union{Timer, AsyncCondition})
                set = t.set
                if set
                    # full barrier now for AsyncCondition
                    t isa Timer || Core.Intrinsics.atomic_fence(:acquire_release, :system)
                else
    - summary: "Extend the atomic_fence intrinsic to accept a syncscope and select LLVM syncscope at compile time when possible."
      component: "Codegen.Intrinsics"
      evidence:
        - source: "code"
          path: "src/intrinsics.cpp"
          loc: "915-938"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/intrinsics.cpp#L915-L938"
          snippet: |
            static jl_cgval_t emit_atomicfence(jl_codectx_t &ctx, ArrayRef<jl_cgval_t> argv)
            {
                const jl_cgval_t &ord = argv[0];
                const jl_cgval_t &ssid_arg = argv[1];
                llvm::SyncScope::ID ssid = llvm::SyncScope::System;
                if (!ssid_arg.constant || !jl_is_symbol(ssid_arg.constant) ||
                    ((jl_sym_t*)ssid_arg.constant != jl_singlethread_sym &&
                        (jl_sym_t*)ssid_arg.constant != jl_system_sym)) {
                    return emit_runtime_call(ctx, atomic_fence, argv, 2);
                }
                if ((jl_sym_t*)ssid_arg.constant == jl_singlethread_sym)
                    ssid = llvm::SyncScope::SingleThread;
                if (ord.constant && jl_is_symbol(ord.constant)) {
                    enum jl_memory_order order = jl_get_atomic_order((jl_sym_t*)ord.constant, true, true);
                    if (order == jl_memory_order_invalid) {
                        emit_atomic_error(ctx, "invalid atomic ordering");
                        return jl_cgval_t(); // unreachable
                    }
                    if (order > jl_memory_order_monotonic)
                        ctx.builder.CreateFence(get_llvm_atomic_order(order), ssid);
                    return ghostValue(ctx, jl_nothing_type);
                }
                return emit_runtime_call(ctx, atomic_fence, argv, 2);
            }
    - summary: "Runtime intrinsic validation adds syncscope checking, a single-thread compiler barrier, and error reporting for invalid syncscope symbols."
      component: "Runtime.Intrinsics"
      evidence:
        - source: "code"
          path: "src/runtime_intrinsics.c"
          loc: "625-638"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/runtime_intrinsics.c#L625-L638"
          snippet: |
            JL_DLLEXPORT jl_value_t *jl_atomic_fence(jl_value_t *order_sym, jl_value_t *syncscope_sym)
            {
                JL_TYPECHK(fence, symbol, order_sym);
                JL_TYPECHK(fence, symbol, syncscope_sym);
                enum jl_memory_order order = jl_get_atomic_order_checked((jl_sym_t*)order_sym, 1, 1);
                if ((jl_sym_t*)syncscope_sym == jl_singlethread_sym) {
                    asm volatile ("" : : : "memory");
                    return jl_nothing;
                } else if ((jl_sym_t*)syncscope_sym != jl_system_sym) {
                    jl_error("atomic_fence: invalid syncscope");
                }
                if (order > jl_memory_order_monotonic)
                    jl_fence();
                return jl_nothing;
            }
    - summary: "Introduce jl_membarrier with OS-specific heavy-fence implementations (Linux/FreeBSD, macOS, Windows)."
      component: "Runtime.Signals"
      evidence:
        - source: "code"
          path: "src/signals-unix.c"
          loc: "1367-1445"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/signals-unix.c#L1367-L1445"
          snippet: |
            enum membarrier_implementation {
                MEMBARRIER_IMPLEMENTATION_UNKNOWN        = 0,
                MEMBARRIER_IMPLEMENTATION_SYS_MEMBARRIER = 1,
                MEMBARRIER_IMPLEMENTATION_MPROTECT       = 2,
                MEMBARRIER_IMPLEMENTATION_THREAD_SUSPEND = 3
            };

            static _Atomic(enum membarrier_implementation) membarrier_impl = MEMBARRIER_IMPLEMENTATION_UNKNOWN;

            static enum membarrier_implementation jl_init_membarrier(void) {
            #ifdef HAVE_MEMBARRIER_SYSCALL
                int ret = membarrier(MEMBARRIER_CMD_QUERY, 0);
                int needed = MEMBARRIER_CMD_PRIVATE_EXPEDITED | MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED;
                if (ret > 0 && ((ret & needed) == needed)) {
                    // supported
                    if (membarrier(MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED, 0) == 0) {
                        // working
                        jl_atomic_store_relaxed(&membarrier_impl, MEMBARRIER_IMPLEMENTATION_SYS_MEMBARRIER);
                        return MEMBARRIER_IMPLEMENTATION_SYS_MEMBARRIER;
                    }
                }
            #endif
                // The mprotect fallback is known not to work on AArch64, so skip it there
            #if !defined(_CPU_AARCH64_) && !defined(_CPU_ARM_)
                if (jl_init_mprotect_membarrier()) {
                    jl_atomic_store_relaxed(&membarrier_impl, MEMBARRIER_IMPLEMENTATION_MPROTECT);
                    return MEMBARRIER_IMPLEMENTATION_MPROTECT;
                }
            #endif
                // Fall back to thread suspension (sound but slow)
                jl_atomic_store_relaxed(&membarrier_impl, MEMBARRIER_IMPLEMENTATION_THREAD_SUSPEND);
                return MEMBARRIER_IMPLEMENTATION_THREAD_SUSPEND;
            }

            JL_DLLEXPORT void jl_membarrier(void) {
                enum membarrier_implementation impl = jl_atomic_load_relaxed(&membarrier_impl);
                if (impl == MEMBARRIER_IMPLEMENTATION_UNKNOWN) {
                    impl = jl_init_membarrier();
                }
                switch (impl) {
            #ifdef HAVE_MEMBARRIER_SYSCALL
                case MEMBARRIER_IMPLEMENTATION_SYS_MEMBARRIER: {
                    int ret = membarrier(MEMBARRIER_CMD_PRIVATE_EXPEDITED, 0);
                    assert(ret == 0);
                    (void)ret;
                    break;
                }
            #endif
            #if !defined(_CPU_AARCH64_) && !defined(_CPU_ARM_)
                case MEMBARRIER_IMPLEMENTATION_MPROTECT:
                    jl_mprotect_membarrier();
                    break;
            #endif
                case MEMBARRIER_IMPLEMENTATION_THREAD_SUSPEND:
                    jl_thread_suspend_membarrier();
                    break;
                default:
                    abort();
                }
            }
        - source: "code"
          path: "src/signals-mach.c"
          loc: "896-937"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/signals-mach.c#L896-L937"
          snippet: |
            // The mprotect implementation in signals-unix.c does not work on macOS/aarch64, as mentioned.
            // This implementation comes from dotnet, but is similarly dependent on undocumented behavior of the OS.
            // Copyright (c) .NET Foundation and Contributors
            // MIT LICENSE
            JL_DLLEXPORT void jl_membarrier(void) {
                uintptr_t sp;
                uintptr_t registerValues[128];
                kern_return_t machret;

                // Iterate through each of the threads in the list.
                int nthreads = jl_atomic_load_acquire(&jl_n_threads);
                for (int tid = 0; tid < nthreads; tid++) {
                    jl_ptls_t ptls2 = jl_atomic_load_relaxed(&jl_all_tls_states)[tid];
                    thread_act_t thread = pthread_mach_thread_np(ptls2->system_id);
                    if (__builtin_available (macOS 10.14, iOS 12, tvOS 9, *))
                    {
                        // Request the threads pointer values to force the thread to emit a memory barrier
                        size_t registers = 128;
                        machret = thread_get_register_pointer_values(thread, &sp, &registers, registerValues);
                    }
                    else
                    {
                        // fallback implementation for older OS versions
            #if defined(_CPU_X86_64_)
                        x86_thread_state64_t threadState;
                        mach_msg_type_number_t count = x86_THREAD_STATE64_COUNT;
                        machret = thread_get_state(thread, x86_THREAD_STATE64, (thread_state_t)&threadState, &count);
            #elif defined(_CPU_AARCH64_)
                        arm_thread_state64_t threadState;
                        mach_msg_type_number_t count = ARM_THREAD_STATE64_COUNT;
                        machret = thread_get_state(thread, ARM_THREAD_STATE64, (thread_state_t)&threadState, &count);
            #else
                        #error Unexpected architecture
            #endif
                    }

                    if (machret == KERN_INSUFFICIENT_BUFFER_SIZE)
                    {
                        HANDLE_MACH_ERROR("thread_get_register_pointer_values()", machret);
                    }
                }
            }
        - source: "code"
          path: "src/signals-win.c"
          loc: "668-670"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/src/signals-win.c#L668-L670"
          snippet: |
            JL_DLLEXPORT void jl_membarrier(void) {
                FlushProcessWriteBuffers();
            }
    - summary: "Compiler tfuncs and intrinsic metadata updated for the new atomic_fence arity and syncscope argument."
      component: "Compiler.tfuncs"
      evidence:
        - source: "code"
          path: "Compiler/src/tfuncs.jl"
          loc: "719-761"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/Compiler/src/tfuncs.jl#L719-L761"
          snippet: |
            @nospecs function atomic_fence_tfunc(ð•ƒ::AbstractLattice, order, syncscope)
                return Nothing
            end
            @nospecs function atomic_pointerref_tfunc(ð•ƒ::AbstractLattice, a, order)
                return pointer_eltype(a)
            end
            @nospecs function atomic_pointerset_tfunc(ð•ƒ::AbstractLattice, a, v, order)
                return a
            end
            @nospecs function atomic_pointerswap_tfunc(ð•ƒ::AbstractLattice, a, v, order)
                return pointer_eltype(a)
            end
            @nospecs function atomic_pointermodify_tfunc(ð•ƒ::AbstractLattice, ptr, op, v, order)
                a = widenconst(ptr)
                if !has_free_typevars(a)
                    unw = unwrap_unionall(a)
                    if isa(unw, DataType) && unw.name === Ptr.body.name
                        T = unw.parameters[1]
                        # note: we could sometimes refine this to a PartialStruct if we analyzed `op(T, T)::T`
                        valid_as_lattice(T, true) || return Bottom
                        return rewrap_unionall(Pair{T, T}, a)
                    end
                end
                return Pair
            end
            @nospecs function atomic_pointerreplace_tfunc(ð•ƒ::AbstractLattice, ptr, x, v, success_order, failure_order)
                a = widenconst(ptr)
                if !has_free_typevars(a)
                    unw = unwrap_unionall(a)
                    if isa(unw, DataType) && unw.name === Ptr.body.name
                        T = unw.parameters[1]
                        valid_as_lattice(T) || return Bottom
                        return rewrap_unionall(ccall(:jl_apply_cmpswap_type, Any, (Any,), T), a)
                    end
                end
                return ccall(:jl_apply_cmpswap_type, Any, (Any,), T) where T
            end
            add_tfunc(add_ptr, 2, 2, pointerarith_tfunc, 1)
            add_tfunc(sub_ptr, 2, 2, pointerarith_tfunc, 1)
            add_tfunc(pointerref, 3, 3, pointerref_tfunc, 4)
            add_tfunc(pointerset, 4, 4, pointerset_tfunc, 5)
            add_tfunc(atomic_fence, 2, 2, atomic_fence_tfunc, 4)
            add_tfunc(atomic_pointerref, 2, 2, atomic_pointerref_tfunc, 4)
            add_tfunc(atomic_pointerset, 3, 3, atomic_pointerset_tfunc, 5)
            add_tfunc(atomic_pointerswap, 3, 3, atomic_pointerswap_tfunc, 5)
    - summary: "Tests updated to exercise new syncscope signature and asymmetric fence behavior."
      component: "Tests.Threading"
      evidence:
        - source: "code"
          path: "test/intrinsics.jl"
          loc: "397-405"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/test/intrinsics.jl#L397-L405"
          snippet: |
            using Base.Experimental: @force_compile
            @test_throws ConcurrencyViolationError("invalid atomic ordering") (@force_compile; Core.Intrinsics.atomic_fence(:u, :system)) === nothing
            @test_throws ConcurrencyViolationError("invalid atomic ordering") (@force_compile; Core.Intrinsics.atomic_fence(Symbol("u", "x"), :system)) === nothing
            @test_throws ConcurrencyViolationError("invalid atomic ordering") Core.Intrinsics.atomic_fence(Symbol("u", "x"), :system) === nothing
            for order in (:not_atomic, :monotonic, :acquire, :release, :acquire_release, :sequentially_consistent)
                @test Core.Intrinsics.atomic_fence(order, :system) === nothing
                @test (order -> Core.Intrinsics.atomic_fence(order, :system))(order) === nothing
                @test Base.invokelatest(@eval () -> Core.Intrinsics.atomic_fence($(QuoteNode(order)), :system)) === nothing
            end
        - source: "code"
          path: "test/threads_exec.jl"
          loc: "467-515"
          url: "https://github.com/JuliaLang/julia/blob/b399d93b9eaf5bd1f223fefbb667e111a824c151/test/threads_exec.jl#L467-L515"
          snippet: |
            # Test asymmetric thread fences
            struct AsymmetricFenceTestData
                n::Int
                x::AtomicMemory{Int}
                y::AtomicMemory{Int}
                read_x::AtomicMemory{Int}
                read_y::AtomicMemory{Int}
            end
            function test_asymmetric_fence(data::AsymmetricFenceTestData, cond1, cond2, threadid, it)
                if (threadid % 2) == 0
                    @atomic :monotonic data.x[it] = 1
                    Threads.atomic_fence_heavy()
                    @atomic :monotonic data.read_y[it] = @atomic :monotonic data.y[it]
                    wait(cond1)
                    notify(cond2)
                else
                    @atomic :monotonic data.y[it] = 1
                    Threads.atomic_fence_light()
                    @atomic :monotonic data.read_x[it] = @atomic :monotonic data.x[it]
                    notify(cond1)
                    wait(cond2)
                end
            end
            function test_asymmetric_fence(data::AsymmetricFenceTestData, cond1, cond2, threadid)
                for i = 1:data.n
                    test_asymmetric_fence(data, cond1, cond2, threadid, i)
                end
            end
            function test_asymmetric_fence()
                asymmetric_test_count = 200_000
                cond1 = Threads.Event(true)
                cond2 = Threads.Event(true)
                data = AsymmetricFenceTestData(asymmetric_test_count,
                                               AtomicMemory{Int}(undef, asymmetric_test_count),
                                               AtomicMemory{Int}(undef, asymmetric_test_count),
                                               AtomicMemory{Int}(undef, asymmetric_test_count),
                                               AtomicMemory{Int}(undef, asymmetric_test_count))
                for i = 1:asymmetric_test_count
                    @atomic :monotonic data.x[i] = 0
                    @atomic :monotonic data.y[i] = 0
                    @atomic :monotonic data.read_x[i] = typemax(Int)
                    @atomic :monotonic data.read_y[i] = typemax(Int)
                end
                t1 = @Threads.spawn test_asymmetric_fence(data, cond1, cond2, 1)
                t2 = @Threads.spawn test_asymmetric_fence(data, cond1, cond2, 2)
                wait(t1); wait(t2)
                @test !any((data.read_x .== 0) .& (data.read_y .== 0))
            end
            test_asymmetric_fence()
  secondary_effects:
    - effect: "Light fences only emit a compiler barrier (single-thread syncscope) and therefore synchronize only with heavy fences, not with ordinary fences."
      mechanism: |
        Threads.atomic_fence_light()  [base/atomics.jl:334-347]
          -> Core.Intrinsics.atomic_fence(:sequentially_consistent, :singlethread)  [base/atomics.jl:347]
          -> emit_atomicfence() chooses llvm::SyncScope::SingleThread and uses CreateFence(order, ssid)  [src/intrinsics.cpp:915-935]
          -> runtime fallback treats :singlethread as `asm volatile ("" : : : "memory")`  [src/runtime_intrinsics.c:625-633]
      downstream_surfaces:
        - "Base.Threads.atomic_fence_light"
        - "Core.Intrinsics.atomic_fence(syncscope=:singlethread)"
        - "LLVM IR fence syncscope=singlethread"
      likelihood: "high"
      impact: "medium"
    - effect: "Heavy fence path may trigger system calls or thread suspension, causing cross-thread latency spikes on some OS/arch combinations."
      mechanism: |
        Threads.atomic_fence_heavy()  [base/atomics.jl:349-356]
          -> ccall(:jl_membarrier, Cvoid, ())  [base/atomics.jl:356]
          -> jl_membarrier() selects implementation (membarrier syscall, mprotect, or thread suspend)  [src/signals-unix.c:1367-1445]
          -> macOS uses Mach thread register queries to induce barriers  [src/signals-mach.c:896-936]
          -> Windows uses FlushProcessWriteBuffers  [src/signals-win.c:668-670]
      downstream_surfaces:
        - "Threads.atomic_fence_heavy"
        - "Runtime jl_membarrier"
        - "Schedulers sensitive to thread suspension"
      likelihood: "medium"
      impact: "medium"
    - effect: "Invalid syncscope symbols now raise errors in runtime intrinsics, which may surface in tooling that calls Core.Intrinsics.atomic_fence directly."
      mechanism: |
        Core.Intrinsics.atomic_fence(order, syncscope)
          -> jl_atomic_fence(order_sym, syncscope_sym)  [src/runtime_intrinsics.c:625-638]
          -> jl_error("atomic_fence: invalid syncscope") for non-:system/:singlethread symbols  [src/runtime_intrinsics.c:633-634]
      downstream_surfaces:
        - "Core.Intrinsics.atomic_fence"
        - "Compiler plugins emitting intrinsics"
      likelihood: "low"
      impact: "low"
  compatibility:
    internal_api:
      - field: "Core.Intrinsics.atomic_fence arity"
        change: "Intrinsic now takes (order, syncscope) instead of a single order argument; metadata updated in intrinsics.h and codegen assert expects nargs==2."
        affected_tools:
          - tool: "GPUCompiler"
            usage: "Emits Core.Intrinsics.atomic_fence when lowering custom IR to Julia IR; must pass syncscope symbol."
          - tool: "IRTools"
            usage: "May construct intrinsic calls; any use of atomic_fence must be updated to 2 args."
      - field: "jl_atomic_fence signature"
        change: "Runtime API now expects (order, syncscope) and validates syncscope symbol before issuing a fence."
        affected_tools:
          - tool: "Embedding APIs"
            usage: "C extensions calling jl_atomic_fence must update call sites to supply syncscope symbol."
      - field: "jl_membarrier export"
        change: "New runtime export available for heavy fence path."
        affected_tools:
          - tool: "Embedding APIs"
            usage: "Could be used by external runtimes to issue process-wide barriers; now exported in jl_exported_funcs.inc."
    behavioral:
      - change: "Base.Threads.atomic_fence now uses syncscope :system explicitly, and a new :singlethread syncscope is introduced for light fences."
        impact: "Direct calls to Core.Intrinsics.atomic_fence must include a valid syncscope or they will error."
      - change: "Asymmetric fence pair guarantees are now tested via a two-thread atomic memory litmus."
        impact: "Code that expects symmetric fence behavior should prefer Threads.atomic_fence() instead of light/heavy pair."
  performance:
    compile_time:
      - item: "ESTIMATED: negligible compile-time impact; extra intrinsic argument and syncscope validation are constant-time in codegen." 
    runtime:
      - item: "ESTIMATED: atomic_fence_light emits only a compiler barrier (singlethread syncscope) and should be much cheaper than a full system fence." 
      - item: "ESTIMATED: atomic_fence_heavy may perform system calls or thread suspension, so per-call latency can be high; use on cold paths." 
  risk:
    level: "medium"
    rationale:
      - "New syncscope argument touches codegen, runtime intrinsics, and exported C API; mistakes could surface as invalid call errors or wrong fence strength."
      - "Heavy fence implementations depend on OS-specific behavior (membarrier/mprotect/thread suspension) with potential latency and portability implications."
  open_questions:
    - "Should additional documentation clarify that atomic_fence_light does not synchronize with Threads.atomic_fence(), only with atomic_fence_heavy?"
    - "Are there performance benchmarks for jl_membarrier fallbacks (mprotect vs thread suspension) on AArch64 Linux/FreeBSD?"
  recommendations:
    - "Audit downstream packages that call Core.Intrinsics.atomic_fence directly to pass an explicit syncscope (:system for ordinary fences)."
    - "Prefer Threads.atomic_fence_light/heavy only when the synchronization pattern is asymmetric; otherwise keep using Threads.atomic_fence()."
    - "Consider adding microbenchmarks for jl_membarrier implementations to detect regressions in latency-sensitive workloads."
