schema_version: "1.0"

pr:
  number: 60105
  title: "Add JLJITLinkMemoryManager (ports memory manager to JITLink)"
  url: "https://github.com/JuliaLang/julia/pull/60105"
  author: "xal-0"
  labels:
    - "performance"
    - "reverted"
  merged_at: "2025-11-13T00:13:14Z"
  merge_commit_sha: "c5c99642cdd11687c7cb4fafc39009694371ae82"
  diff_url: "https://github.com/JuliaLang/julia/pull/60105.diff"
  related_prs:
    - number: 60031
      description: "Switch to JITLink everywhere - this PR addresses memory regressions from that change"
    - number: 60196
      description: "Revert of this PR due to aarch64-darwin deadlock"
    - number: 60245
      description: "Follow-up fix that disables JLJITLinkMemoryManager on non-large code model platforms"
  reverted: true
  revert_pr: 60196
  revert_merged_at: "2025-11-22T01:20:18Z"
  revert_reason: |
    Nightly builds of aarch64-darwin Julia hung at startup on GitHub Actions and local M1 Macs.
    The hang was a deadlock in pthread mutex/condition variable operations. The revert was
    bisected to PR 60105 by giordano. Error message showed:
      __psynch_cvwait at /usr/lib/system/libsystem_kernel.dylib
      __psynch_mutexwait at /usr/lib/system/libsystem_kernel.dylib

scope:
  files_touched:
    - "src/cgmemmgr.cpp"
    - "src/jitlayers.cpp"
  components:
    - "Compiler.Codegen"
  pipeline_stages:
    - "Codegen"

analysis:
  intent:
    summary: |
      This PR ports Julia's custom RTDyLD memory manager to JITLink in order to avoid memory use
      regressions after PR #60031 switched Julia to use JITLink everywhere. JITLink is LLVM's
      newer runtime JIT linker that replaces the older RuntimeDyld (RTDyLD).

      The key motivation is that the default LLVM InProcessMemoryMapper used with JITLink has
      higher memory overhead compared to Julia's custom memory management. By porting the
      existing optimizations (dual-mapped memory, /proc/self/mem tricks, memory pooling) to
      work with JITLink, Julia can maintain efficient memory usage after the JITLink transition.

      CRITICAL: This PR was reverted 9 days after merge due to deadlock on macOS ARM64 (M1/M2).
      A follow-up fix (PR 60245) was merged that disables JLJITLinkMemoryManager on platforms
      using non-large code models (aarch64, riscv64).
    issue_links: []
    quoted_from_pr: |
      "Ports our RTDyLD memory manager to JITLink in order to avoid memory use regressions
      after switching to JITLink everywhere (#60031). This is a direct port: finalization must
      happen all at once, because it invalidates all allocation wr_ptrs. I decided it wasn't
      worth it to associate OnFinalizedFunction callbacks with each block, since they are large
      enough to make it extremely likely that all in-flight allocations land in the same block;
      everything must be relocated before finalization can happen."

  direct_changes:
    - summary: "Add new JLJITLinkMemoryManager class implementing JITLink memory management"
      component: "Compiler.Codegen"
      evidence:
        - source: "diff"
          path: "src/cgmemmgr.cpp"
          loc: "931-1057"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L931-L1057"
          snippet: |
            class JLJITLinkMemoryManager : public jitlink::JITLinkMemoryManager {
                using OnFinalizedFunction =
                    jitlink::JITLinkMemoryManager::InFlightAlloc::OnFinalizedFunction;

                std::mutex Mutex;
                RWAllocator RWAlloc;
                std::unique_ptr<ROAllocator> ROAlloc;
                std::unique_ptr<ROAllocator> ExeAlloc;
                SmallVector<OnFinalizedFunction> FinalizedCallbacks;
                uint32_t InFlight{0};

            public:
                class InFlightAlloc;

                static std::unique_ptr<JITLinkMemoryManager> Create()
                {
                    auto [ROAlloc, ExeAlloc] = get_preferred_allocators();
                    if (ROAlloc && ExeAlloc)
                        return std::unique_ptr<JLJITLinkMemoryManager>(
                            new JLJITLinkMemoryManager(std::move(ROAlloc), std::move(ExeAlloc)));

                    return cantFail(
                        orc::MapperJITLinkMemoryManager::CreateWithMapper<orc::InProcessMemoryMapper>(
                            /*Reservation Granularity*/ 16 * 1024 * 1024));
                }

    - summary: "Refactor RWAllocator::alloc to return Allocation struct instead of void*"
      component: "Compiler.Codegen"
      evidence:
        - source: "diff"
          path: "src/cgmemmgr.cpp"
          loc: "481-497"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L481-L497"
          snippet: |
            Allocation alloc(size_t size, size_t align) JL_NOTSAFEPOINT
            {
                size_t min_size = (size_t)-1;
                int min_id = 0;
                for (int i = 0;i < nblocks && blocks[i].ptr;i++) {
                    if (void *ptr = blocks[i].alloc(size, align))
                        return {ptr, ptr, size, false};
                    if (blocks[i].avail < min_size) {
                        min_size = blocks[i].avail;
                        min_id = i;
                    }
                }
                size_t block_size = get_block_size(size);
                blocks[min_id].reset(map_anon_page(block_size), block_size);
                void *ptr = blocks[min_id].alloc(size, align);
                return {ptr, ptr, size, false};
            }

    - summary: "Refactor ROAllocator::alloc to return Allocation struct instead of void*"
      component: "Compiler.Codegen"
      evidence:
        - source: "diff"
          path: "src/cgmemmgr.cpp"
          loc: "563-611"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L563-L611"
          snippet: |
            Allocation alloc(size_t size, size_t align) JL_NOTSAFEPOINT
            {
                size_t min_size = (size_t)-1;
                int min_id = 0;
                for (int i = 0;i < nblocks && blocks[i].ptr;i++) {
                    auto &block = blocks[i];
                    void *ptr = block.alloc(size, align);
                    if (ptr) {
                        void *wr_ptr;
                        if (block.state & SplitPtrBlock::InitAlloc) {
                            wr_ptr = ptr;
                        }
                        else {
                            wr_ptr = get_wr_ptr(block, ptr, size, align);
                        }
                        block.state |= SplitPtrBlock::Alloc;
                        Allocation a{wr_ptr, ptr, size, false};
                        allocations.push_back(a);
                        return a;
                    }

    - summary: "Remove template parameter from ROAllocator classes, make exec a runtime parameter"
      component: "Compiler.Codegen"
      evidence:
        - source: "diff"
          path: "src/cgmemmgr.cpp"
          loc: "614-677"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L614-L677"
          snippet: |
            class DualMapAllocator : public ROAllocator {
                bool exec;

            protected:
                void *get_wr_ptr(SplitPtrBlock &block, void *rt_ptr, size_t, size_t) override JL_NOTSAFEPOINT
                {
                    assert((char*)rt_ptr >= block.ptr &&
                           (char*)rt_ptr < (block.ptr + block.total));
                    if (!(block.state & SplitPtrBlock::WRInit)) {
                        block.wr_ptr = (uintptr_t)create_shared_map(block.total,
                                                                    block.wr_ptr);
                        block.state |= SplitPtrBlock::WRInit;
                    }
                }
            public:
                DualMapAllocator(bool exec) JL_NOTSAFEPOINT : exec(exec)
                {
                    assert(anon_hdl != -1);
                }

    - summary: "Add get_preferred_allocators factory function"
      component: "Compiler.Codegen"
      evidence:
        - source: "diff"
          path: "src/cgmemmgr.cpp"
          loc: "772-784"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L772-L784"
          snippet: |
            std::pair<std::unique_ptr<ROAllocator>, std::unique_ptr<ROAllocator>>
            get_preferred_allocators() JL_NOTSAFEPOINT
            {
            #ifdef _OS_LINUX_
                if (get_self_mem_fd() != -1)
                    return {std::make_unique<SelfMemAllocator>(false),
                            std::make_unique<SelfMemAllocator>(true)};
            #endif
                if (init_shared_map() != -1)
                    return {std::make_unique<DualMapAllocator>(false),
                            std::make_unique<DualMapAllocator>(true)};
                return {};
            }

    - summary: "Move createJITLinkMemoryManager from jitlayers.cpp to cgmemmgr.cpp"
      component: "Compiler.Codegen"
      evidence:
        - source: "diff"
          path: "src/jitlayers.cpp"
          loc: "1234"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/jitlayers.cpp#L1234"
          snippet: |
            std::unique_ptr<jitlink::JITLinkMemoryManager> createJITLinkMemoryManager() JL_NOTSAFEPOINT;
        - source: "diff"
          path: "src/cgmemmgr.cpp"
          loc: "1070-1073"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L1070-L1073"
          snippet: |
            std::unique_ptr<jitlink::JITLinkMemoryManager> createJITLinkMemoryManager()
            {
                return JLJITLinkMemoryManager::Create();
            }

    - summary: "JLJITLinkMemoryManager::allocate handles segment allocation by memory protection type"
      component: "Compiler.Codegen"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "1021-1057"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L1021-L1057"
          snippet: |
            void JLJITLinkMemoryManager::allocate(const jitlink::JITLinkDylib *JD,
                                                  jitlink::LinkGraph &G,
                                                  OnAllocatedFunction OnAllocated)
            {
                jitlink::BasicLayout BL{G};

                {
                    std::unique_lock Lock{Mutex};
                    for (auto &[AG, Seg] : BL.segments()) {
                        if (AG.getMemLifetime() == orc::MemLifetime::NoAlloc)
                            continue;
                        assert(AG.getMemLifetime() == orc::MemLifetime::Standard);

                        auto Prot = AG.getMemProt();
                        uint64_t Alignment = Seg.Alignment.value();
                        uint64_t Size = Seg.ContentSize + Seg.ZeroFillSize;
                        Allocation Alloc;
                        if (Prot == (MemProt::Read | MemProt::Write))
                            Alloc = RWAlloc.alloc(Size, Alignment);
                        else if (Prot == MemProt::Read)
                            Alloc = ROAlloc->alloc(Size, Alignment);
                        else if (Prot == (MemProt::Read | MemProt::Exec))
                            Alloc = ExeAlloc->alloc(Size, Alignment);
                        else
                            abort();

                        Seg.Addr = orc::ExecutorAddr::fromPtr(Alloc.rt_addr);
                        Seg.WorkingMem = (char *)Alloc.wr_addr;
                    }
                }

                if (auto Err = BL.apply())
                    return OnAllocated(std::move(Err));

                ++InFlight;  // NOTE: Incremented OUTSIDE the lock - potential race condition
                OnAllocated(std::make_unique<InFlightAlloc>(*this, G));
            }

    - summary: "JLJITLinkMemoryManager::finalize batches all in-flight allocations"
      component: "Compiler.Codegen"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "973-990"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L973-L990"
          snippet: |
            void finalize(OnFinalizedFunction OnFinalized)
            {
                SmallVector<OnFinalizedFunction> Callbacks;
                {
                    std::unique_lock Lock{Mutex};
                    FinalizedCallbacks.push_back(std::move(OnFinalized));

                    if (--InFlight > 0)  // NOTE: Decremented INSIDE the lock - asymmetric with allocate()
                        return;

                    ROAlloc->finalize();
                    ExeAlloc->finalize();
                    Callbacks = std::move(FinalizedCallbacks);
                }

                for (auto &CB : Callbacks)
                    std::move(CB)(FinalizedAlloc{});
            }

  secondary_effects:
    - effect: "CRITICAL: Deadlock on macOS ARM64 during JIT initialization"
      mechanism: |
        The revert PR 60196 identified a deadlock on aarch64-darwin causing Julia to hang at startup.
        Symptoms observed on M1 Mac with macOS 12.6 and GitHub Actions macOS runners:

        Error backtrace showed:
          __psynch_cvwait at /usr/lib/system/libsystem_kernel.dylib
          __psynch_mutexwait at /usr/lib/system/libsystem_kernel.dylib

        Root cause analysis from code review:
        1. allocate() increments InFlight OUTSIDE the lock [cgmemmgr.cpp:1055]
        2. finalize() decrements InFlight INSIDE the lock [cgmemmgr.cpp:980]
        3. This asymmetric locking can lead to race conditions where:
           - Thread A calls allocate(), releases lock, but hasn't incremented InFlight yet
           - Thread B calls finalize(), acquires lock, decrements InFlight (potentially to -1 as uint32_t)
           - This causes undefined behavior or deadlock in the batched finalization logic

        The specific deadlock manifested as Julia hanging during startup on ARM64 macOS,
        likely when the JIT compiler was initializing and compiling core functions.
      downstream_surfaces:
        - "All aarch64-darwin builds (Apple Silicon M1/M2/M3)"
        - "GitHub Actions macOS ARM64 runners"
        - "CI pipelines running Julia nightly on macOS ARM64"
      likelihood: "high"
      impact: "critical"
      evidence:
        - source: "revert_pr"
          path: "https://github.com/JuliaLang/julia/pull/60196"
          loc: "body"
          snippet: |
            Reverts JuliaLang/julia#60105. Nightly builds of aarch64-darwin Julia hang at
            startup on some systems, notably on GitHub Actions, making all nightly jobs
            timeout (after 6 hours...).

            The error message when the process receives a SIGTERM signal is:
            in expression starting at none:0
            __psynch_cvwait at /usr/lib/system/libsystem_kernel.dylib (unknown line)
            unknown function (ip: 0x0) at (unknown file)
            __psynch_mutexwait at /usr/lib/system/libsystem_kernel.dylib (unknown line)

            I can reliably reproduce it locally on a M1 box with MacOSX 12.6 21.6.0

    - effect: "Code model incompatibility with pooled allocator on ARM64/RISC-V"
      mechanism: |
        Follow-up PR 60245 revealed another fundamental issue:

        On platforms using non-large code models (aarch64, riscv64), the JITLink linker
        requires relocations to be within a certain address range. Julia's pooled memory
        allocator can place allocations far apart in address space, causing relocation
        failures when the linker tries to fix up references between code and data.

        Call chain where failure occurs:
        1. JLJITLinkMemoryManager::allocate() allocates segments from pool  [cgmemmgr.cpp:1027-1049]
        2. Segments may be placed in different memory blocks far apart
        3. BL.apply() or subsequent linking step tries to fix relocations  [cgmemmgr.cpp:1052]
        4. Relocation fails if distance exceeds code model limits
        5. JITLink calls abandon() which hits jl_unreachable() and crashes

        PR 60245 fixed this by:
        1. Disabling JLJITLinkMemoryManager on aarch64 and riscv64 platforms
        2. Removing jl_unreachable() from deallocate() and abandon() to get better error messages
      downstream_surfaces:
        - "ARM64 Linux builds"
        - "ARM64 macOS builds"
        - "RISC-V builds"
      likelihood: "high"
      impact: "high"
      evidence:
        - source: "follow_up_pr"
          path: "https://github.com/JuliaLang/julia/pull/60245"
          loc: "body"
          snippet: |
            Under normal circumstances we never deallocate JITted code, and JITLink will
            never abandon in-flight allocations. Unfortunately, on platforms where we use
            a code model that require relocations to not be too large, the linker can bail
            out if an allocation gets placed somewhere unlucky.

            This change forces the use of MapperJITLinkMemoryManager on the platforms where
            we use a non-large code model so that this can't happen, and removes the
            jl_unreachable() calls so that the JITLink error gets reported nicely instead
            of failing in our code.
        - source: "follow_up_pr_diff"
          path: "src/cgmemmgr.cpp"
          loc: "771-784"
          snippet: |
            std::pair<std::unique_ptr<ROAllocator>, std::unique_ptr<ROAllocator>>
            get_preferred_allocators() JL_NOTSAFEPOINT
            {
            #if !(defined(_CPU_AARCH64_) || defined(_CPU_RISCV64_))
            #ifdef _OS_LINUX_
                if (get_self_mem_fd() != -1)
                    return {std::make_unique<SelfMemAllocator>(false),
                            std::make_unique<SelfMemAllocator>(true)};
            #endif
                if (init_shared_map() != -1)
                    return {std::make_unique<DualMapAllocator>(false),
                            std::make_unique<DualMapAllocator>(true)};
            #endif
                return {};
            }

    - effect: "Memory allocation strategy changes for JITLink users"
      mechanism: |
        The call chain for JIT memory allocation changes:

        JuliaOJIT constructor  [jitlayers.cpp:1922-1924]
          -> createJITLinkMemoryManager()  [cgmemmgr.cpp:1070-1073]
          -> JLJITLinkMemoryManager::Create()  [cgmemmgr.cpp:945-955]
          -> get_preferred_allocators()  [cgmemmgr.cpp:772-784]
            On Linux: tries SelfMemAllocator (uses /proc/self/mem)
            On other platforms: tries DualMapAllocator (uses shared mmap)
            Fallback: MapperJITLinkMemoryManager with InProcessMemoryMapper

        When preferred allocators are available, JITLink now uses Julia's memory pools
        which are more compact than LLVM's default reservation-based approach.
      downstream_surfaces:
        - "All JIT-compiled code when JL_USE_JITLINK is defined"
        - "ARM64 architectures (JITLink is always used)"
        - "RISC-V architectures (JITLink is always used)"
        - "Builds with sanitizers (JITLink is always used)"
      likelihood: "high"
      impact: "medium"
      evidence:
        - source: "code"
          path: "src/jitlayers.h"
          loc: "58-64"
          url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/jitlayers.h#L58-L64"
          snippet: |
            #if defined(JL_FORCE_JITLINK) || defined(_CPU_AARCH64_) || defined(HAS_SANITIZER)
            # define JL_USE_JITLINK
            #endif

            #if defined(_CPU_RISCV64_)
            # define JL_USE_JITLINK
            #endif

    - effect: "Finalization batching changes timing semantics"
      mechanism: |
        The design comment in the PR explains: "finalization must happen all at once,
        because it invalidates all allocation wr_ptrs". This means:

        1. JLJITLinkMemoryManager tracks InFlight allocations  [cgmemmgr.cpp:940]
        2. On allocate(), InFlight is incremented  [cgmemmgr.cpp:1055]
        3. On finalize(), InFlight is decremented  [cgmemmgr.cpp:980]
        4. Only when InFlight reaches 0 does actual finalization happen  [cgmemmgr.cpp:980-985]
        5. All registered callbacks are then invoked  [cgmemmgr.cpp:988-989]

        ISSUE: The asymmetric locking (increment outside lock, decrement inside) creates
        a potential race condition that manifested as deadlock on macOS ARM64.
      downstream_surfaces:
        - "Concurrent JIT compilation behavior"
        - "Thread-safety of JIT emission"
      likelihood: "high"
      impact: "critical"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "973-990"
          snippet: |
            void finalize(OnFinalizedFunction OnFinalized)
            {
                SmallVector<OnFinalizedFunction> Callbacks;
                {
                    std::unique_lock Lock{Mutex};
                    FinalizedCallbacks.push_back(std::move(OnFinalized));

                    if (--InFlight > 0)
                        return;

                    ROAlloc->finalize();
                    ExeAlloc->finalize();
                    Callbacks = std::move(FinalizedCallbacks);
                }

                for (auto &CB : Callbacks)
                    std::move(CB)(FinalizedAlloc{});
            }

    - effect: "Fallback to LLVM MapperJITLinkMemoryManager when preferred allocators unavailable"
      mechanism: |
        JLJITLinkMemoryManager::Create() has a fallback path:

        1. get_preferred_allocators() attempts platform-specific allocators  [cgmemmgr.cpp:772-784]
        2. On Linux: SelfMemAllocator requires kernel >= 4.10  [cgmemmgr.cpp:338]
        3. On other platforms: DualMapAllocator requires init_shared_map() success
        4. If both fail, returns empty pair  [cgmemmgr.cpp:783]
        5. Create() falls back to MapperJITLinkMemoryManager  [cgmemmgr.cpp:952-954]

        This ensures Julia can still function on platforms where the optimized memory
        management isn't available, just with potentially higher memory usage.
      downstream_surfaces:
        - "Julia on older Linux kernels (< 4.10)"
        - "Julia on platforms without shared memory support"
        - "ARM64 macOS (explicitly mentioned in PR as not yet supported)"
      likelihood: "medium"
      impact: "low"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "945-955"
          snippet: |
            static std::unique_ptr<JITLinkMemoryManager> Create()
            {
                auto [ROAlloc, ExeAlloc] = get_preferred_allocators();
                if (ROAlloc && ExeAlloc)
                    return std::unique_ptr<JLJITLinkMemoryManager>(
                        new JLJITLinkMemoryManager(std::move(ROAlloc), std::move(ExeAlloc)));

                return cantFail(
                    orc::MapperJITLinkMemoryManager::CreateWithMapper<orc::InProcessMemoryMapper>(
                        /*Reservation Granularity*/ 16 * 1024 * 1024));
            }

    - effect: "Memory pool sharing between RTDyLD and JITLink code paths"
      mechanism: |
        The refactoring creates shared infrastructure:

        Before PR:
          - RTDyldMemoryManagerJL created its own allocators in constructor  [old jitlayers.cpp]
          - JITLink used MapperJITLinkMemoryManager directly  [old jitlayers.cpp:1208-1213]

        After PR:
          - Both use get_preferred_allocators()  [cgmemmgr.cpp:772-784]
          - RTDyldMemoryManagerJL: std::tie(ro_alloc, exe_alloc) = get_preferred_allocators()  [cgmemmgr.cpp:806]
          - JLJITLinkMemoryManager: auto [ROAlloc, ExeAlloc] = get_preferred_allocators()  [cgmemmgr.cpp:947]

        This code sharing improves maintainability but also means changes to allocator
        behavior affect both code paths.
      downstream_surfaces:
        - "RTDyLD-based JIT (on x86_64 Linux/Windows/macOS without sanitizers)"
        - "JITLink-based JIT (on ARM64, RISC-V, or with sanitizers)"
      likelihood: "high"
      impact: "low"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "806"
          snippet: |
            std::tie(ro_alloc, exe_alloc) = get_preferred_allocators();

    - effect: "Crash on deallocation or abandonment due to jl_unreachable()"
      mechanism: |
        The PR implements deallocate() and abandon() as:

          void deallocate(...) override { jl_unreachable(); }
          void abandon(...) override { jl_unreachable(); }

        This causes immediate crash if JITLink ever needs to clean up allocations.
        As PR 60245 discovered, on non-large code model platforms, relocation failures
        can trigger abandon() calls, leading to crashes instead of error handling.

        Call chain to crash:
        1. JITLink fails to fix relocation (address too far)  [JITLink internal]
        2. JITLink calls InFlightAlloc::abandon()  [cgmemmgr.cpp:1001]
        3. jl_unreachable() is called  [cgmemmgr.cpp:1001]
        4. Julia crashes with assertion failure or abort

        PR 60245 fixed this by removing jl_unreachable() to allow JITLink's error
        reporting to provide useful diagnostics.
      downstream_surfaces:
        - "Error handling paths in JIT compilation"
        - "Platforms with non-large code models (ARM64, RISC-V)"
      likelihood: "medium"
      impact: "high"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "960-964"
          snippet: |
            void deallocate(std::vector<FinalizedAlloc> Allocs,
                            OnDeallocatedFunction OnDeallocated) override
            {
                jl_unreachable();
            }
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "1001"
          snippet: |
            void abandon(OnAbandonedFunction OnAbandoned) override { jl_unreachable(); }

    - effect: "InFlightAlloc runs finalize actions after memory finalization"
      mechanism: |
        The InFlightAlloc::finalize method:

        1. Calls MM.finalize with a callback  [cgmemmgr.cpp:1006]
        2. Callback runs AFTER memory finalization completes  [cgmemmgr.cpp:1007-1014]
        3. Runs orc::shared::runFinalizeActions(GP->allocActions())  [cgmemmgr.cpp:1011]
        4. Propagates errors properly  [cgmemmgr.cpp:1008-1009, 1012-1013]

        The comment notes: "Need to handle dealloc actions when we GC code" - suggesting
        this is preparation for future code GC support in Julia.
      downstream_surfaces:
        - "Future code garbage collection implementation"
        - "LLVM ORC finalize action hooks"
      likelihood: "low"
      impact: "low"
      evidence:
        - source: "code"
          path: "src/cgmemmgr.cpp"
          loc: "1003-1016"
          snippet: |
            void finalize(OnFinalizedFunction OnFinalized) override
            {
                auto *GP = &G;
                MM.finalize([GP, OnFinalized =
                                     std::move(OnFinalized)](Expected<FinalizedAlloc> FA) mutable {
                    if (!FA)
                        return OnFinalized(FA.takeError());
                    // Need to handle dealloc actions when we GC code
                    auto E = orc::shared::runFinalizeActions(GP->allocActions());
                    if (!E)
                        return OnFinalized(E.takeError());
                    OnFinalized(std::move(FA));
                });
            }

  compatibility:
    internal_api:
      - summary: "Allocation struct moved and made public to both RTDyLD and JITLink"
        evidence:
          - source: "diff"
            path: "src/cgmemmgr.cpp"
            loc: "467-474"
            snippet: |
              struct Allocation {
                  // Address to write to (the one returned by the allocation function)
                  void *wr_addr;
                  // Runtime address
                  void *rt_addr;
                  size_t sz;
                  bool relocated;
              };
      - summary: "RWAllocator::alloc return type changed from void* to Allocation"
        evidence:
          - source: "diff"
            path: "src/cgmemmgr.cpp"
            loc: "481"
            snippet: |
              Allocation alloc(size_t size, size_t align) JL_NOTSAFEPOINT
      - summary: "ROAllocator::alloc return type changed from void* to Allocation"
        evidence:
          - source: "diff"
            path: "src/cgmemmgr.cpp"
            loc: "563"
            snippet: |
              Allocation alloc(size_t size, size_t align) JL_NOTSAFEPOINT
      - summary: "Template parameter <bool exec> removed from ROAllocator, DualMapAllocator, SelfMemAllocator"
        evidence:
          - source: "diff"
            path: "src/cgmemmgr.cpp"
            loc: "536, 614, 695"
            snippet: |
              class ROAllocator {  // was: template<bool exec> class ROAllocator
              class DualMapAllocator : public ROAllocator {  // was: template<bool exec>
              class SelfMemAllocator : public ROAllocator {  // was: template<bool exec>
    behavioral:
      - summary: "JITLink memory allocation now uses Julia's pooled memory management"
        evidence:
          - source: "code"
            path: "src/cgmemmgr.cpp"
            loc: "1021-1057"
            snippet: |
              void JLJITLinkMemoryManager::allocate(...)

  performance:
    compile_time:
      - summary: "ESTIMATED: Negligible change to compilation time"
        evidence:
          - source: "analysis"
            path: "N/A"
            snippet: |
              The PR changes memory allocation strategy but not the compilation
              algorithms. Memory allocation overhead should be similar to the
              existing RTDyLD code path. The batched finalization may slightly
              delay when code becomes executable, but this is offset by reduced
              memory management overhead.
    runtime:
      - summary: "ESTIMATED: No runtime impact after compilation"
        evidence:
          - source: "analysis"
            path: "N/A"
            snippet: |
              This change only affects JIT compilation. Once code is compiled
              and finalized, there is no difference in execution speed.
    memory:
      - summary: "ESTIMATED: Reduced memory usage compared to default JITLink allocator"
        evidence:
          - source: "diff"
            path: "src/cgmemmgr.cpp"
            loc: "952-954"
            snippet: |
              // Default allocator reserves 16MB at a time
              orc::MapperJITLinkMemoryManager::CreateWithMapper<orc::InProcessMemoryMapper>(
                  /*Reservation Granularity*/ 16 * 1024 * 1024)
          - source: "analysis"
            path: "N/A"
            snippet: |
              The default InProcessMemoryMapper reserves memory in 16MB chunks.
              Julia's ROAllocator uses block sizes of min(size, 256 * page_size),
              which is typically ~1MB. This results in significantly lower memory
              overhead for workloads with many small functions.

              Additionally, Julia's SelfMemAllocator (Linux) and DualMapAllocator
              use specialized OS primitives that can be more memory-efficient than
              generic memory mapping.

  tests:
    changed_files: []
    new_behavior_assertions: []
    coverage_gaps:
      - "No tests added for JLJITLinkMemoryManager"
      - "PR relies on existing JIT tests passing to validate correctness"
      - "Memory usage reduction not validated with specific benchmarks"
      - "No tests for concurrent compilation scenarios that exposed the deadlock"
      - "No tests for error handling paths (deallocate/abandon)"

  risk:
    level: "critical"
    rationale:
      - "CONFIRMED: PR was reverted 9 days after merge due to deadlock on macOS ARM64"
      - "Deadlock caused Julia nightly builds to hang for 6+ hours on GitHub Actions"
      - "Race condition in InFlight counter management (increment outside lock, decrement inside)"
      - "Code model incompatibility causes relocation failures on ARM64/RISC-V"
      - "jl_unreachable() in deallocate/abandon causes crashes instead of error handling"
      - "No dedicated tests added for the new code paths"
      - "Platform-specific bugs difficult to reproduce on all developer machines"
      - "JIT memory management is critical infrastructure - bugs can cause crashes or memory corruption"

  open_questions:
    - "ANSWERED: Why was this PR reverted? Deadlock on aarch64-darwin due to mutex race condition"
    - "ANSWERED: Follow-up PR 60245 disabled JLJITLinkMemoryManager on non-large code model platforms"
    - "What is the correct fix for the InFlight counter race condition?"
    - "Should the increment be moved inside the lock to match the decrement?"
    - "How to properly test concurrent JIT compilation scenarios?"

  recommendations:
    - "Move ++InFlight inside the mutex lock to match the decrement and avoid race conditions"
    - "Add extensive tests for concurrent JIT compilation on all platforms"
    - "Test specifically on macOS ARM64 (M1/M2) before any resubmission"
    - "Implement proper deallocate() and abandon() methods with leak tracking instead of jl_unreachable()"
    - "Consider disabling JLJITLinkMemoryManager on ARM64/RISC-V as done in PR 60245"
    - "Add CI jobs that test JIT compilation under stress/concurrent workloads"
    - "Add memory usage benchmarks to validate the claimed improvements"

changelog_entry:
  category: "Codegen/JIT"
  breaking: false
  summary: |
    Ported Julia's custom memory manager from RTDyLD to JITLink to reduce memory usage
    when using LLVM's newer JIT linker. This affects ARM64, RISC-V, and sanitizer builds
    which use JITLink.

    CRITICAL: This change was reverted 9 days after merge (PR #60196) due to deadlock
    on macOS ARM64. A follow-up fix (PR #60245) was merged that disables the custom
    allocator on ARM64 and RISC-V platforms.
  downstream_impact: |
    This change primarily affects Julia's internal JIT compilation infrastructure.
    Most downstream packages should be unaffected. However, any tooling that depends
    on specific JIT memory layout or timing assumptions may need to be tested.

    The revert means this change is not in current Julia master. Follow-up PR 60245
    provides a safer implementation that uses the default LLVM allocator on ARM64/RISC-V.

downstream_package_impact:
  Turing_jl: "none - JIT memory management is transparent to Julia packages"
  Enzyme_jl: "potentially affected - relies heavily on JIT compilation for AD transforms; would have been affected by the deadlock on ARM64 macOS"
  GPUCompiler: "potentially affected - uses JIT infrastructure for GPU code generation; ARM64 builds would have been affected"
  JET: "none - static analysis does not depend on JIT memory management"

code_path_trace:
  jit_initialization:
    description: "How JITLinkMemoryManager is created and installed"
    steps:
      - location: "src/jitlayers.cpp:1922-1924"
        url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/jitlayers.cpp#L1922-L1924"
        code: |
          #ifdef JL_USE_JITLINK
              MemMgr(createJITLinkMemoryManager()),
              ObjectLayer(ES, *MemMgr),
        explanation: |
          In JuliaOJIT constructor, createJITLinkMemoryManager() is called to create
          the memory manager, which is then passed to the ObjectLinkingLayer.

      - location: "src/cgmemmgr.cpp:1070-1073"
        url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L1070-L1073"
        code: |
          std::unique_ptr<jitlink::JITLinkMemoryManager> createJITLinkMemoryManager()
          {
              return JLJITLinkMemoryManager::Create();
          }
        explanation: |
          createJITLinkMemoryManager() is a factory function that delegates to
          JLJITLinkMemoryManager::Create().

      - location: "src/cgmemmgr.cpp:945-955"
        url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L945-L955"
        code: |
          static std::unique_ptr<JITLinkMemoryManager> Create()
          {
              auto [ROAlloc, ExeAlloc] = get_preferred_allocators();
              if (ROAlloc && ExeAlloc)
                  return std::unique_ptr<JLJITLinkMemoryManager>(
                      new JLJITLinkMemoryManager(std::move(ROAlloc), std::move(ExeAlloc)));

              return cantFail(
                  orc::MapperJITLinkMemoryManager::CreateWithMapper<orc::InProcessMemoryMapper>(
                      /*Reservation Granularity*/ 16 * 1024 * 1024));
          }
        explanation: |
          Create() first tries to get Julia's preferred allocators (SelfMemAllocator
          on Linux, DualMapAllocator otherwise). If those fail, it falls back to
          LLVM's default MapperJITLinkMemoryManager.

  allocation_flow:
    description: "How memory is allocated for JIT code"
    steps:
      - location: "src/cgmemmgr.cpp:1021-1057"
        url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L1021-L1057"
        code: |
          void JLJITLinkMemoryManager::allocate(const jitlink::JITLinkDylib *JD,
                                                jitlink::LinkGraph &G,
                                                OnAllocatedFunction OnAllocated)
          {
              jitlink::BasicLayout BL{G};

              {
                  std::unique_lock Lock{Mutex};
                  for (auto &[AG, Seg] : BL.segments()) {
                      // ... allocate based on protection flags
                      Seg.Addr = orc::ExecutorAddr::fromPtr(Alloc.rt_addr);
                      Seg.WorkingMem = (char *)Alloc.wr_addr;
                  }
              }
              // LOCK RELEASED HERE

              if (auto Err = BL.apply())
                  return OnAllocated(std::move(Err));

              ++InFlight;  // BUG: Incremented OUTSIDE lock - race condition!
              OnAllocated(std::make_unique<InFlightAlloc>(*this, G));
          }
        explanation: |
          allocate() is called by LLVM's JITLink when it needs memory for compiled code.
          It iterates through segments, allocates appropriate memory based on protection
          (RW for data, RO for read-only, RX for executable), and tracks in-flight
          allocations for batched finalization.

          BUG: The ++InFlight happens AFTER the lock is released, creating a race
          condition with finalize() which decrements InFlight INSIDE the lock.

  finalization_flow:
    description: "How memory is finalized and made executable"
    steps:
      - location: "src/cgmemmgr.cpp:1003-1016"
        url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L1003-L1016"
        code: |
          void finalize(OnFinalizedFunction OnFinalized) override
          {
              auto *GP = &G;
              MM.finalize([GP, OnFinalized =
                                   std::move(OnFinalized)](Expected<FinalizedAlloc> FA) mutable {
                  if (!FA)
                      return OnFinalized(FA.takeError());
                  auto E = orc::shared::runFinalizeActions(GP->allocActions());
                  if (!E)
                      return OnFinalized(E.takeError());
                  OnFinalized(std::move(FA));
              });
          }
        explanation: |
          InFlightAlloc::finalize() delegates to JLJITLinkMemoryManager::finalize(),
          then runs LLVM's finalize actions and calls the completion callback.

      - location: "src/cgmemmgr.cpp:973-990"
        url: "https://github.com/JuliaLang/julia/blob/c5c99642cdd11687c7cb4fafc39009694371ae82/src/cgmemmgr.cpp#L973-L990"
        code: |
          void finalize(OnFinalizedFunction OnFinalized)
          {
              SmallVector<OnFinalizedFunction> Callbacks;
              {
                  std::unique_lock Lock{Mutex};
                  FinalizedCallbacks.push_back(std::move(OnFinalized));

                  if (--InFlight > 0)  // BUG: Decremented INSIDE lock - asymmetric with allocate()!
                      return;

                  ROAlloc->finalize();
                  ExeAlloc->finalize();
                  Callbacks = std::move(FinalizedCallbacks);
              }

              for (auto &CB : Callbacks)
                  std::move(CB)(FinalizedAlloc{});
          }
        explanation: |
          JLJITLinkMemoryManager::finalize() batches finalizations. It decrements
          InFlight and only performs actual finalization (setting memory protections)
          when all in-flight allocations have requested finalization.

          BUG: The --InFlight happens INSIDE the lock, while allocate() increments
          OUTSIDE the lock. This asymmetry creates a race condition that can cause
          deadlock or incorrect counter values.

  deadlock_scenario:
    description: "Race condition leading to deadlock on macOS ARM64"
    steps:
      - location: "Race condition analysis"
        code: |
          Thread A:                              Thread B:
          --------                               --------
          allocate() called
          Lock acquired
          Memory allocated
          Lock released
          // InFlight not yet incremented!
                                                 finalize() called
                                                 Lock acquired
                                                 --InFlight  // Was 0, now wraps to UINT32_MAX or -1!
                                                 if (InFlight > 0) return  // RETURNS, never finalizes
                                                 // Lock held but finalize() already returned
          ++InFlight  // Now 1
          // Thread A expects callbacks but they never come

          Or alternatively:
          - InFlight becomes corrupted due to unsynchronized access
          - Callbacks vector gets into inconsistent state
          - ROAlloc->finalize() or ExeAlloc->finalize() hangs
        explanation: |
          The asymmetric locking between allocate() and finalize() creates multiple
          potential race conditions. On macOS ARM64, this manifested as a deadlock
          where Julia hung at startup waiting on pthread mutex/condvar operations.

reviewer_notes:
  initial_reviewer: "automated_analysis"
  initial_date: "2026-01-21"
  secondary_reviewer: "claude_independent_analysis"
  secondary_date: "2026-01-21"
  verification_method: |
    Initial review:
    1. Read PR metadata from pr-archive/JuliaLang_julia/pr_60105.json
    2. Cloned Julia repo and checked out merge commit c5c99642cdd11687c7cb4fafc39009694371ae82
    3. Read full source of src/cgmemmgr.cpp (1074 lines)
    4. Read relevant sections of src/jitlayers.cpp and src/jitlayers.h
    5. Traced call chains from JuliaOJIT constructor through memory allocation
    6. Analyzed allocator class hierarchy (RWAllocator, ROAllocator, DualMapAllocator, SelfMemAllocator)
    7. Identified platform-specific code paths (Linux /proc/self/mem, shared mmap)
    8. Noted the "reverted" label indicating post-merge issues

    Secondary review (independent analysis):
    1. Examined revert PR 60196 to identify actual cause of failure
    2. Examined follow-up fix PR 60245 to understand additional issues
    3. Identified race condition in InFlight counter management
    4. Traced deadlock scenario caused by asymmetric locking
    5. Identified code model incompatibility with pooled allocator on ARM64/RISC-V
    6. Analyzed jl_unreachable() crash risk in error handling paths
  findings:
    - "This PR ports RTDyLD memory management optimizations to JITLink"
    - "Key changes: JLJITLinkMemoryManager class, unified allocator factory, batched finalization"
    - "Platform-specific optimizations: SelfMemAllocator on Linux, DualMapAllocator elsewhere"
    - "Fallback to default LLVM allocator if Julia's allocators fail to initialize"
    - "CONFIRMED: PR was reverted due to deadlock on aarch64-darwin (macOS ARM64)"
    - "ROOT CAUSE: Race condition - InFlight incremented outside lock, decremented inside lock"
    - "SECONDARY ISSUE: Code model incompatibility causes relocation failures on ARM64/RISC-V"
    - "TERTIARY ISSUE: jl_unreachable() in deallocate/abandon causes crashes"
    - "No tests added for the new JLJITLinkMemoryManager code"
    - "Follow-up PR 60245 disables JLJITLinkMemoryManager on ARM64/RISC-V"
  confidence: "very high"
  rationale: |
    The analysis is enhanced with concrete evidence from the revert PR and follow-up fix:
    - PR 60196 provides exact reproduction steps and error messages for the deadlock
    - PR 60245 explains the code model incompatibility and provides the fix
    - Independent code review identified the asymmetric locking as the race condition
    - The deadlock scenario is documented step-by-step
    - Multiple secondary effects are now understood and documented
